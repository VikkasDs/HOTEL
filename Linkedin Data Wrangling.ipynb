{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "PH-0ReGfmX4f",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "JcMwzZxoAimU",
        "8G2x9gOozGDZ",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Vikash Tiwari\n",
        "##### **Team Member 1 -**\n",
        "##### **Team Member 2 -**\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   There were 119390 rows & 32 columns in the actual dataset\n",
        "*   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have you ever wondered when the best time of year to book a hotel room is? Or the optimal length of stay in order to get the best daily rate? What if you wanted to predict whether or not a hotel was likely to receive a disproportionately high number of special requests? This hotel booking dataset can help you explore those questions! This data set contains booking information for a city hotel and a resort hotel, and includes information such as when the booking was made, length of stay, the number of adults, children, and/or babies, and the number of available parking spaces, among other things. All personally identifying information has been removed from the data. Explore and analyze the data to discover important factors that govern the bookings."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Define Your Business Objective?**"
      ],
      "metadata": {
        "id": "PH-0ReGfmX4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysing the data of Resort & City Hotel to gain insights on various factors like Best time to book hotel, optimal length of stay to get best rates,time when hotels can expect maximum & minimum bookings,factors that lead to hotel cancellations"
      ],
      "metadata": {
        "id": "PhDvGCAqmjP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 20 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#HANDLING MISSING DATA\n",
        "#STARDARDIZIG DATA FORMATS\n",
        "#FILTER UNWANTED OUTLIERS\n",
        "#HANDLING DUPLICATES"
      ],
      "metadata": {
        "id": "bleSfmO13LvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import missingno as msno\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#Loading Linkedin dataset\n",
        "data= '/content/drive/MyDrive/Robin Sharma Data/AB ROBIN SHARMA LINKEDIN POST DATA.csv'\n",
        "Linkedin_data= pd.read_csv(data)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Linkedin Dataset First Look\n",
        "Linkedin_data"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "rows, columns= Linkedin_data.shape\n",
        "print(f'there are {rows} Rows & {columns} Columns in this dataset')"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "Linkedin_data.info()\n"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>1. Handling Duplicate Values**"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "duplicate = Linkedin_data.duplicated()\n",
        "#creating a boolean Series object called duplicate by calling the duplicated() method on the Linkedin_data DataFrame"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate"
      ],
      "metadata": {
        "id": "RgSlfy1amQyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filtering the Linkedin_data DataFrame using the boolean Series 'duplicate' & storig that in duplicate_rows. This would only display duplicate values\n",
        "duplicate_rows= Linkedin_data[duplicate]\n"
      ],
      "metadata": {
        "id": "zluxYivVoSca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_rows\n",
        "#there are no duplicate rows in this dataframe"
      ],
      "metadata": {
        "id": "1CFu3uv2qktv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>2. Handling Missing Values/Null Values**"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "missing_values= Linkedin_data.isnull().sum().sort_values(ascending=False)\n",
        "missing_values\n",
        "#This dataframe had 106 null values in video_length column,53 nulls in hashtags_used columns,14 nulls in tags_used,"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating bar plot\n",
        "plt.figure(figsize=(10,6))\n",
        "#creating a color pallete\n",
        "palette = sns.color_palette(\"husl\", len(missing_values))\n",
        "bars = plt.bar(missing_values.index, missing_values, color=palette)\n",
        "plt.title('Missing Values')\n",
        "plt.xlabel('Columns')\n",
        "# Rotate x-axis labels for better readability\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "#Lets Annotate the bar plot with missing value counts\n",
        "for bar in bars:\n",
        "  height=bar.get_height()\n",
        "  plt.text(\n",
        "      bar.get_x() + bar.get_width() / 2,\n",
        "      height,\n",
        "      f'{int(height)}',\n",
        "      ha='center',\n",
        "      va='bottom'\n",
        "  )\n",
        "  #using plt.text() which a function from matplotlib library that adds text to the plot at specified coordinates\n",
        "plt.ylabel('No of Missing Values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6xHPucFiEY3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.There are total 189 Rows & 16 Columns in this dataset.                         \n",
        "2.Company column has 94% Null data & agent column has 14% Null data.Country column has only .4% null data & children column has only 4 null data.                        \n",
        "3.There are 4 types of meal booked BB,FB,HB,SC                                         \n",
        "4.There are total 31994 missing values\n"
      ],
      "metadata": {
        "id": "8NztVXDTzzaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "Linkedin_data.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "Linkedin_data.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for elements in Linkedin_data.columns:\n",
        "  print('No of Unique values in',elements,'column is',Linkedin_data[elements].nunique())\n",
        "\n",
        "##Here element represents each column"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "#Missing values\n",
        "Linkedin_data.isnull().sum().sort_values(ascending=False)[:8]"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#replacing null values of video_length column with 0\n",
        "Linkedin_data[['video_length(Mins)']]=Linkedin_data[['video_length(Mins)']].fillna(0)"
      ],
      "metadata": {
        "id": "bmATdCUwyE-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#replacing null values of hashtags_used column with 'none'\n",
        "Linkedin_data['hashtags_used']=Linkedin_data['hashtags_used'].fillna('none')"
      ],
      "metadata": {
        "id": "kwx-08VOyge5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Linkedin_data.columns"
      ],
      "metadata": {
        "id": "Avi6jdScoDc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying hastags_used & tags_used column\n",
        "specific_columns = Linkedin_data[['hashtags_used','tags_used']]\n",
        "specific_columns\n"
      ],
      "metadata": {
        "id": "26okmmWdQWmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We have to update hastgas_used columns with Yes or No if hashtags are used or not used. Some hashtags_used rows have 'none' even though tags have been used\n",
        "\n",
        "def add_hashtags_used(Linkedin_data):\n",
        "    # Replace 'none' and 'NO' with blank (empty string)\n",
        "    Linkedin_data['tags_used'] = Linkedin_data['tags_used'].replace(['NONE'], '', inplace=True)"
      ],
      "metadata": {
        "id": "Nt09G2VgVq2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T2_Gu1eHUlpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking whether above function have changed all the hashtags_used column data or not\n",
        "Linkedin_data"
      ],
      "metadata": {
        "id": "0p8DyJR1Bd6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_hashtags_used(Linkedin_data):\n",
        "    # Replace 'none' and 'NO' with blank (empty string)\n",
        "    Linkedin_data['tags_used'] = Linkedin_data['tags_used'].replace(['none', 'NO'], '')\n",
        "    # Create 'hashtags_used' column based on 'tags_used'\n",
        "    Linkedin_data['hashtags_used'] = Linkedin_data['tags_used'].apply(lambda x: 'YES' if pd.notna(x) and x != '' else 'NO')\n",
        "    return Linkedin_data"
      ],
      "metadata": {
        "id": "pfTTb37FV2Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking for anymore left null data\n",
        "Linkedin_data.isnull().sum().sort_values(ascending=False)[:8]"
      ],
      "metadata": {
        "id": "cE1hK087ysw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To fill all null values in the repost column with 0, you can use the fillna method\n",
        "# Fill null values in 'repost' column with 0\n",
        "Linkedin_data['repost'].fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "4YJJsSJPirIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display only the rows where 'repost' column has null values\n",
        "null_repost_rows = Linkedin_data[Linkedin_data['repost'].isnull()]\n",
        "null_repost_rows"
      ],
      "metadata": {
        "id": "yV9-7ASay4Uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display only the rows where 'tags_used' column has null values\n",
        "null_repost_rows = Linkedin_data[Linkedin_data['tags_used'].isnull()]\n",
        "null_repost_rows"
      ],
      "metadata": {
        "id": "g5dV_-OXjEf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display only the rows where 'type' column has null values\n",
        "null_repost_rows = Linkedin_data[Linkedin_data['type'].isnull()]\n",
        "null_repost_rows"
      ],
      "metadata": {
        "id": "_V7KQTXC3gO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Filling all null 'type' column data with 'Motivation'\n",
        "Linkedin_data['type'].fillna('Motivation',inplace=True)"
      ],
      "metadata": {
        "id": "X2eOrQ0wmyfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking anymore null type rows are left or not\n",
        "null_type_rows = Linkedin_data[Linkedin_data['type'].isnull()]\n",
        "null_type_rows"
      ],
      "metadata": {
        "id": "VGjbPOe7nS5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display only the rows where 'likes' column has null values\n",
        "null_likes_rows = Linkedin_data[Linkedin_data['likes'].isnull()]\n",
        "null_likes_rows"
      ],
      "metadata": {
        "id": "v9nT4Zux5uUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping these 4 rows\n",
        "Linkedin_data.dropna(subset=['likes'], inplace=True)\n",
        "null_likes_rows = Linkedin_data[Linkedin_data['likes'].isnull()]\n",
        "null_likes_rows\n",
        "#So, now there are null 'likes' rows"
      ],
      "metadata": {
        "id": "1WhH0Rz6oqHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking for anymore left null data\n",
        "Linkedin_data.isnull().sum().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "PmMsnxD1qNxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filling null values of 'influencer_comments' column with zero\n",
        "Linkedin_data['influencer_comments'].fillna(0,inplace=True)"
      ],
      "metadata": {
        "id": "6jBblLHpqHCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking for anymore left null data\n",
        "Linkedin_data.isnull().sum().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "_NoTWIHMuYA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#null values in tags column indicates that author havent used any hastags in those posts. so it need not be changed"
      ],
      "metadata": {
        "id": "91HmfcQoud9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>3) Standardizing Data Formats**"
      ],
      "metadata": {
        "id": "8EcEZmo3unek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking Data types of all columns\n",
        "Linkedin_data.info()"
      ],
      "metadata": {
        "id": "1m2xEuEb6Kdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'post_date' column is of object data type & has to be changed to datetime format\n",
        "Linkedin_data[\"post_date\"] = pd.to_datetime(Linkedin_data[\"post_date\"],dayfirst=True)"
      ],
      "metadata": {
        "id": "Ad-vI5yBAUJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Linkedin_data[\"post_date\"]"
      ],
      "metadata": {
        "id": "pUHfcwrv37Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting post_data from datetime format to string to remove any extra space & remove any unwanted characters\n",
        "Linkedin_data[\"post_date\"] = Linkedin_data[\"post_date\"].astype(str)"
      ],
      "metadata": {
        "id": "8eGJxUpwW_Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_dates(date_str):\n",
        "    for fmt in ('%d-%b-%y', '%d-%b-%Y', '%d %b %Y'):\n",
        "        try:\n",
        "            return pd.to_datetime(date_str, format=fmt, dayfirst=True)\n",
        "        except ValueError:\n",
        "            continue\n",
        "    return pd.NaT  # return NaT if none of the formats match\n",
        "\n",
        "# Apply the custom date parser function to the 'post_date' column\n",
        "Linkedin_data[\"parsed_post_date\"] = Linkedin_data[\"post_date\"].apply(parse_dates)\n",
        "\n",
        "# Identify rows with faulty 'post_date' data\n",
        "faulty_dates = Linkedin_data[Linkedin_data[\"parsed_post_date\"].isna()]\n",
        "\n",
        "# Display rows with faulty 'post_date' data\n",
        "print(\"Rows with faulty 'post_date' data:\\n\", faulty_dates)"
      ],
      "metadata": {
        "id": "rmRELmKVWcpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking Data types of all columns\n",
        "Linkedin_data.info()"
      ],
      "metadata": {
        "id": "_eGnA8GA1ldo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting post_date from string data type to datetime\n",
        "Linkedin_data[\"post_date\"] = pd.to_datetime(Linkedin_data[\"post_date\"],dayfirst=True)\n",
        "#THERE WERE ERRORS HERE\n",
        "#errors='coerce': This is a parameter of the pd.to_datetime() function.\n",
        "#When errors='coerce' is specified, it tells Pandas to set any errors it encounters during conversion to NaT (Not a Time).\n",
        "#NaT is a special value in Pandas that represents missing or undefined datetime values.\n",
        "#This parameter is crucial because it allows the function to handle cases where the input data is not formatted as expected or contains invalid dates without throwing an error.\n",
        "#Instead of halting the conversion process, Pandas will skip over problematic entries and mark them as NaT.\n",
        "#WE DID THE CONVERSION I THE NEXT STEP"
      ],
      "metadata": {
        "id": "NuTG1CkT0Fom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting post_date from string data type to datetime\n",
        "Linkedin_data[\"post_date\"] = pd.to_datetime(Linkedin_data[\"post_date\"], errors='coerce')\n",
        "Linkedin_data.info()"
      ],
      "metadata": {
        "id": "1_44M5jFA0Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting post_time to time datatype\n",
        "import re\n",
        "\n",
        "def clean_time_string(time_str):\n",
        "    # Remove leading and trailing spaces\n",
        "    time_str = time_str.strip()\n",
        "    # Remove unwanted characters (keep only digits and colons)\n",
        "    time_str = re.sub(r'[^0-9:]', '', time_str)\n",
        "    return time_str\n",
        "\n",
        "# Apply the cleaning function to the 'time' column\n",
        "Linkedin_data['cleaned_time'] = Linkedin_data['post_time'].apply(clean_time_string)\n",
        "print(\"\\nCleaned Time Strings:\")\n",
        "print(Linkedin_data)\n",
        "\n",
        "# Function to convert cleaned time strings to datetime.time\n",
        "def convert_to_time(time_str):\n",
        "    try:\n",
        "        dt = datetime.strptime(time_str, \"%H:%M:%S\")  # Parse the time string\n",
        "        return dt.time()  # Extract the time part\n",
        "    except ValueError:\n",
        "        return None  # Handle invalid formats\n",
        "\n",
        "# Apply the conversion function to the cleaned 'cleaned_time' column\n",
        "Linkedin_data['cl_post_time'] = Linkedin_data['cleaned_time'].apply(convert_to_time)\n",
        "print(\"\\nConverted Time Objects:\")\n",
        "print(Linkedin_data)"
      ],
      "metadata": {
        "id": "4HtAnxwHBaht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Linkedin_data.info()"
      ],
      "metadata": {
        "id": "BZaCwxnzQRAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Linkedin_data['cl_post_time'] = pd.to_datetime(Linkedin_data['cl_post_time'], format='%H:%M:%S', errors='coerce').dt.time\n",
        "Linkedin_data.info()"
      ],
      "metadata": {
        "id": "AcUXA_HaB9zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Linkedin_data[Linkedin_data['cl_post_time'].isnull()])  # Check rows where conversion failed\n",
        "#THIS OUTPUT SHOWS THAT ALL CONVERSION HAVE BEEN DONE SUCCESSFULLY\n",
        "#WE WOULD BE USING THIS COLUMN AS IT IS . IT IS CLEANED COMPLETELY\n"
      ],
      "metadata": {
        "id": "C4Hd3aknCj55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we would drop parsed_post_date,post_time,cl_post_date\n",
        "columns_to_drop = ['parsed_post_date','post_time','cleaned_time']\n",
        "Linkedin_data.drop(columns=columns_to_drop,inplace=True)\n",
        ""
      ],
      "metadata": {
        "id": "GJH1zVq5h7iV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Linkedin_data.info()"
      ],
      "metadata": {
        "id": "HMcDAD1OiKrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Linkedin_data.drop(columns=['cl_post_date'],inplace=True)\n"
      ],
      "metadata": {
        "id": "mqTPQTJUVx0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Linkedin_data.info()"
      ],
      "metadata": {
        "id": "8vsJzOJ-WDGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking whether the sum column is dropped or not\n",
        "df_without_duplicates.columns\n",
        "df_without_duplicates.shape"
      ],
      "metadata": {
        "id": "zsvhgmezinyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Following Manipulations have been done:           \n",
        "\n",
        "1. Reservation_status_date column data type have been changed to datetime from object\n",
        "\n",
        "2. New column named **Total_stay** have been created adding weekend & week night stays column. Subsequently, these two columns have been dropped then.\n",
        "3.New column total_kids have been created adding babies & children column. Subsequently babies & children have been dropped after that.\n",
        "4.We dropped 180 rows from adults, babies & children column as there were having 0 values\n",
        "5.We found company column had 94% missing values, agent column had 14% missing values,country column had 488 null values & babies column had 4 null values.Company,babies,agent column null values were replacd by 0 & country column missing values were replaced by Others\n",
        "6.There were 31980 duplicate values in this dataset. All of them were dropped."
      ],
      "metadata": {
        "id": "h7dptbAUMdHh"
      }
    }
  ]
}
