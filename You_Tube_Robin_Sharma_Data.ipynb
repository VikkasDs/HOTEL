{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "PH-0ReGfmX4f",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "JcMwzZxoAimU",
        "8G2x9gOozGDZ",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Vikash Tiwari\n",
        "##### **Team Member 1 -**\n",
        "##### **Team Member 2 -**\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   There were 119390 rows & 32 columns in the actual dataset\n",
        "*   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have you ever wondered when the best time of year to book a hotel room is? Or the optimal length of stay in order to get the best daily rate? What if you wanted to predict whether or not a hotel was likely to receive a disproportionately high number of special requests? This hotel booking dataset can help you explore those questions! This data set contains booking information for a city hotel and a resort hotel, and includes information such as when the booking was made, length of stay, the number of adults, children, and/or babies, and the number of available parking spaces, among other things. All personally identifying information has been removed from the data. Explore and analyze the data to discover important factors that govern the bookings."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Define Your Business Objective?**"
      ],
      "metadata": {
        "id": "PH-0ReGfmX4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysing the data of Resort & City Hotel to gain insights on various factors like Best time to book hotel, optimal length of stay to get best rates,time when hotels can expect maximum & minimum bookings,factors that lead to hotel cancellations"
      ],
      "metadata": {
        "id": "PhDvGCAqmjP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 20 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#HANDLING MISSING DATA\n",
        "#STARDARDIZIG DATA FORMATS\n",
        "#FILTER UNWANTED OUTLIERS\n",
        "#HANDLING DUPLICATES"
      ],
      "metadata": {
        "id": "bleSfmO13LvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import missingno as msno\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#Loading Linkedin dataset\n",
        "data= '/content/drive/MyDrive/ROBIN SHARMA/Final_CL_Robin_Sharma_YouTube_Data.csv'\n",
        "Youtube_data= pd.read_csv(data)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Youtube Dataset First Look\n",
        "Youtube_data"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "rows, columns= Youtube_data.shape\n",
        "print(f'there are {rows} Rows & {columns} Columns in this dataset')"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "Youtube_data.info()\n"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Changing Column names**"
      ],
      "metadata": {
        "id": "IyK0i3TmkVia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#changing 'Unnamed: 0' column name to 'SR. No'\n",
        "Youtube_data.rename(columns={'Unnamed: 0':'SR. No'},inplace=True)"
      ],
      "metadata": {
        "id": "HaFpiLwSkTAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#changing 'Unnamed: 3' column name to 'Published_Date'\n",
        "Youtube_data.rename(columns={'Unnamed: 3':'Published_date'},inplace=True)"
      ],
      "metadata": {
        "id": "xVUiCKvIlbhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking whether column name have been successfully changed or not\n",
        "Youtube_data.info()"
      ],
      "metadata": {
        "id": "VFQfxXxJluHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>1. Handling Duplicate Values**"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "duplicate = Youtube_data.duplicated()\n",
        "#creating a boolean Series object called duplicate by calling the duplicated() method on the Linkedin_data DataFrame"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate"
      ],
      "metadata": {
        "id": "RgSlfy1amQyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filtering the Linkedin_data DataFrame using the boolean Series 'duplicate' & storig that in duplicate_rows. This would only display duplicate values\n",
        "duplicate_rows= Youtube_data[duplicate]\n"
      ],
      "metadata": {
        "id": "zluxYivVoSca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_rows\n",
        "#there are no duplicate rows in this dataframe"
      ],
      "metadata": {
        "id": "1CFu3uv2qktv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>2. Handling Missing Values/Null Values**"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "missing_values= Youtube_data.isnull().sum().sort_values(ascending=False)\n",
        "missing_values\n",
        "#This dataframe dont have any missing values"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating bar plot\n",
        "plt.figure(figsize=(10,6))\n",
        "#creating a color pallete\n",
        "palette = sns.color_palette(\"husl\", len(missing_values))\n",
        "bars = plt.bar(missing_values.index, missing_values, color=palette)\n",
        "plt.title('Missing Values')\n",
        "plt.xlabel('Columns')\n",
        "# Rotate x-axis labels for better readability\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "#Lets Annotate the bar plot with missing value counts\n",
        "for bar in bars:\n",
        "  height=bar.get_height()\n",
        "  plt.text(\n",
        "      bar.get_x() + bar.get_width() / 2,\n",
        "      height,\n",
        "      f'{int(height)}',\n",
        "      ha='center',\n",
        "      va='bottom'\n",
        "  )\n",
        "  #using plt.text() which a function from matplotlib library that adds text to the plot at specified coordinates\n",
        "plt.ylabel('No of Missing Values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6xHPucFiEY3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "Youtube_data.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "# The Describe method in pandas would give you standard statistics data\n",
        "Youtube_data.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for elements in Youtube_data.columns:\n",
        "  print('No of Unique values in',elements,'column is',Youtube_data[elements].nunique())\n",
        "\n",
        "##Here element represents each column"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "#Missing values\n",
        "Youtube_data.isnull().sum().sort_values(ascending=False)\n",
        "#Thus, there are no null values"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Youtube_data.columns"
      ],
      "metadata": {
        "id": "Avi6jdScoDc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>3) Standardizing Data Formats**"
      ],
      "metadata": {
        "id": "8EcEZmo3unek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking Data types of all columns\n",
        "Youtube_data.info()"
      ],
      "metadata": {
        "id": "1m2xEuEb6Kdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'Published_date' column is of object data type & has to be changed to datetime format\n",
        "Youtube_data[\"Published_date\"] = pd.to_datetime(Youtube_data[\"Published_date\"],dayfirst=True)"
      ],
      "metadata": {
        "id": "Ad-vI5yBAUJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Youtube_data[\"Published_date\"]"
      ],
      "metadata": {
        "id": "pUHfcwrv37Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting 'Published_time' from object datatype to time datatype\n",
        "Youtube_data['Published_time'] = pd.to_datetime(Youtube_data['Published_time'], format='%I:%M:%S %p')\n",
        "#the above step would give us time with default date of '1900-01-01' .\n"
      ],
      "metadata": {
        "id": "8eGJxUpwW_Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Youtube_data['Published_time']"
      ],
      "metadata": {
        "id": "jSXzO4e0JMaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we would be using time extractor from the result to ignore date and get only time\n",
        "Youtube_data['Published_time'] = Youtube_data['Published_time'].dt.time"
      ],
      "metadata": {
        "id": "SPzLVjVKKkxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Youtube_data['Published_time']"
      ],
      "metadata": {
        "id": "7rcn06i6Kuw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_dates(date_str):\n",
        "    for fmt in ('%d-%b-%y', '%d-%b-%Y', '%d %b %Y'):\n",
        "        try:\n",
        "            return pd.to_datetime(date_str, format=fmt, dayfirst=True)\n",
        "        except ValueError:\n",
        "            continue\n",
        "    return pd.NaT  # return NaT if none of the formats match\n",
        "\n",
        "# Apply the custom date parser function to the 'post_date' column\n",
        "Linkedin_data[\"parsed_post_date\"] = Linkedin_data[\"post_date\"].apply(parse_dates)\n",
        "\n",
        "# Identify rows with faulty 'post_date' data\n",
        "faulty_dates = Linkedin_data[Linkedin_data[\"parsed_post_date\"].isna()]\n",
        "\n",
        "# Display rows with faulty 'post_date' data\n",
        "print(\"Rows with faulty 'post_date' data:\\n\", faulty_dates)"
      ],
      "metadata": {
        "id": "rmRELmKVWcpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking Data types of all columns\n",
        "Linkedin_data.info()"
      ],
      "metadata": {
        "id": "_eGnA8GA1ldo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting post_date from string data type to datetime\n",
        "Linkedin_data[\"post_date\"] = pd.to_datetime(Linkedin_data[\"post_date\"],dayfirst=True)\n",
        "#THERE WERE ERRORS HERE\n",
        "#errors='coerce': This is a parameter of the pd.to_datetime() function.\n",
        "#When errors='coerce' is specified, it tells Pandas to set any errors it encounters during conversion to NaT (Not a Time).\n",
        "#NaT is a special value in Pandas that represents missing or undefined datetime values.\n",
        "#This parameter is crucial because it allows the function to handle cases where the input data is not formatted as expected or contains invalid dates without throwing an error.\n",
        "#Instead of halting the conversion process, Pandas will skip over problematic entries and mark them as NaT.\n",
        "#WE DID THE CONVERSION IN THE NEXT STEP"
      ],
      "metadata": {
        "id": "NuTG1CkT0Fom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting post_date from string data type to datetime\n",
        "Linkedin_data[\"post_date\"] = pd.to_datetime(Linkedin_data[\"post_date\"], errors='coerce')\n",
        "Linkedin_data.info()"
      ],
      "metadata": {
        "id": "1_44M5jFA0Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting post_time to time datatype\n",
        "import re\n",
        "\n",
        "def clean_time_string(time_str):\n",
        "    # Remove leading and trailing spaces\n",
        "    time_str = time_str.strip()\n",
        "    # Remove unwanted characters (keep only digits and colons)\n",
        "    time_str = re.sub(r'[^0-9:]', '', time_str)\n",
        "    return time_str\n",
        "\n",
        "# Apply the cleaning function to the 'time' column\n",
        "Linkedin_data['cleaned_time'] = Linkedin_data['post_time'].apply(clean_time_string)\n",
        "print(\"\\nCleaned Time Strings:\")\n",
        "print(Linkedin_data)\n",
        "\n",
        "# Function to convert cleaned time strings to datetime.time\n",
        "def convert_to_time(time_str):\n",
        "    try:\n",
        "        dt = datetime.strptime(time_str, \"%H:%M:%S\")  # Parse the time string\n",
        "        return dt.time()  # Extract the time part\n",
        "    except ValueError:\n",
        "        return None  # Handle invalid formats\n",
        "\n",
        "# Apply the conversion function to the cleaned 'cleaned_time' column\n",
        "Linkedin_data['cl_post_time'] = Linkedin_data['cleaned_time'].apply(convert_to_time)\n",
        "print(\"\\nConverted Time Objects:\")\n",
        "print(Linkedin_data)"
      ],
      "metadata": {
        "id": "4HtAnxwHBaht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Linkedin_data.info()"
      ],
      "metadata": {
        "id": "BZaCwxnzQRAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Linkedin_data['cl_post_time'] = pd.to_datetime(Linkedin_data['cl_post_time'], format='%H:%M:%S', errors='coerce').dt.time\n",
        "Linkedin_data.info()"
      ],
      "metadata": {
        "id": "AcUXA_HaB9zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Linkedin_data[Linkedin_data['cl_post_time'].isnull()])  # Check rows where conversion failed\n",
        "#THIS OUTPUT SHOWS THAT ALL CONVERSION HAVE BEEN DONE SUCCESSFULLY\n",
        "#WE WOULD BE USING THIS COLUMN AS IT IS . IT IS CLEANED COMPLETELY\n"
      ],
      "metadata": {
        "id": "C4Hd3aknCj55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we would drop parsed_post_date,post_time,cl_post_date\n",
        "columns_to_drop = ['parsed_post_date','post_time','cleaned_time']\n",
        "Linkedin_data.drop(columns=columns_to_drop,inplace=True)\n"
      ],
      "metadata": {
        "id": "GJH1zVq5h7iV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Linkedin_data.info()"
      ],
      "metadata": {
        "id": "HMcDAD1OiKrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting Cl_post_time which was of object datatype to datetime datatype\n",
        "Linkedin_data['cl_post_time'] = pd.to_datetime(Linkedin_data['cl_post_time'], format='%H:%M:%S')\n"
      ],
      "metadata": {
        "id": "mqTPQTJUVx0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Linkedin_data.info()"
      ],
      "metadata": {
        "id": "8vsJzOJ-WDGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> 1. There are 189 Rows & 16 Columns in this dataset\n",
        "\n",
        "<h1> 2. There were no duplicate data in this dataset\n",
        "                      \n",
        "<h1>3. This dataframe had 106 null values in video_length column,53 nulls in hashtags_used columns,14 nulls in tags_used\n",
        "\n",
        "<h1>4. Replaced null values of video_length column with 0\n",
        "\n",
        "<h1>5. Replaced null values of hashtags_used column with 'none'"
      ],
      "metadata": {
        "id": "h7dptbAUMdHh"
      }
    }
  ]
}